{"model": "mistralai/Mistral-7B-Instruct-v0.2", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 32000, "max_position_embeddings": 32768, "hidden_size": 4096, "intermediate_size": 14336, "num_hidden_layers": 32, "num_attention_heads": 32, "sliding_window": null, "num_key_value_heads": 8, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "use_cache": true, "rope_theta": 1000000.0, "attention_dropout": 0.0, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["MistralForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 1, "pad_token_id": null, "eos_token_id": 2, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.2", "transformers_version": "4.42.3", "model_type": "mistral"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0, "num_readable_responses": 0, "input_tokens_avg": 0}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.59375, "num_readable_responses": 128, "input_tokens_avg": 875.1328125}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.6328125, "num_readable_responses": 128, "input_tokens_avg": 1035.390625}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.7142857142857143, "num_readable_responses": 42, "input_tokens_avg": 912.7142857142857}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.7487437185929648, "num_readable_responses": 199, "input_tokens_avg": 775.9698492462312}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.6722689075630253, "num_readable_responses": 119, "input_tokens_avg": 927.2352941176471}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.7083333333333334, "num_readable_responses": 24, "input_tokens_avg": 1074.5}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.5111111111111111, "num_readable_responses": 45, "input_tokens_avg": 1166.8444444444444}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.5, "num_readable_responses": 6, "input_tokens_avg": 1101.0}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.6388888888888888, "num_readable_responses": 36, "input_tokens_avg": 1103.5555555555557}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.5714285714285714, "num_readable_responses": 35, "input_tokens_avg": 995.0857142857143}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.7777777777777778, "num_readable_responses": 9, "input_tokens_avg": 1202.4444444444443}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.5505050505050505, "num_readable_responses": 198, "input_tokens_avg": 665.939393939394}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.4215686274509804, "num_readable_responses": 102, "input_tokens_avg": 1095.5098039215686}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.6538461538461539, "num_readable_responses": 52, "input_tokens_avg": 729.75}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 1.0, "num_readable_responses": 2, "input_tokens_avg": 919.5}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.5581395348837209, "num_readable_responses": 43, "input_tokens_avg": 1406.4418604651162}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.3333333333333333, "num_readable_responses": 39, "input_tokens_avg": 812.7435897435897}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.751004016064257, "num_readable_responses": 249, "input_tokens_avg": 1107.3333333333333}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.775, "num_readable_responses": 40, "input_tokens_avg": 1044.05}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.7592592592592593, "num_readable_responses": 54, "input_tokens_avg": 1081.351851851852}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.8076923076923077, "num_readable_responses": 26, "input_tokens_avg": 3793.8846153846152}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.8100558659217877, "num_readable_responses": 179, "input_tokens_avg": 730.754189944134}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.877906976744186, "num_readable_responses": 172, "input_tokens_avg": 987.9651162790698}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.6174496644295302, "num_readable_responses": 298, "input_tokens_avg": 924.4563758389262}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0, "num_readable_responses": 0, "input_tokens_avg": 0}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.7423312883435583, "num_readable_responses": 163, "input_tokens_avg": 847.3251533742331}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.40816326530612246, "num_readable_responses": 49, "input_tokens_avg": 1238.5102040816328}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.8870967741935484, "num_readable_responses": 62, "input_tokens_avg": 830.2903225806451}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.6272727272727273, "num_readable_responses": 110, "input_tokens_avg": 1378.5363636363636}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.7978723404255319, "num_readable_responses": 188, "input_tokens_avg": 3132.372340425532}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.7777777777777778, "num_readable_responses": 63, "input_tokens_avg": 3554.15873015873}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.6666666666666666, "num_readable_responses": 210, "input_tokens_avg": 699.6761904761905}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.8018018018018018, "num_readable_responses": 111, "input_tokens_avg": 755.7927927927927}, "international_law": {"num_questions_subject": 121, "accuracy": 0.7964601769911505, "num_readable_responses": 113, "input_tokens_avg": 1166.9380530973451}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.7623762376237624, "num_readable_responses": 101, "input_tokens_avg": 911.7029702970297}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.7741935483870968, "num_readable_responses": 93, "input_tokens_avg": 874.258064516129}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.75, "num_readable_responses": 16, "input_tokens_avg": 887.375}, "management": {"num_questions_subject": 103, "accuracy": 0.7722772277227723, "num_readable_responses": 101, "input_tokens_avg": 585.4158415841584}, "marketing": {"num_questions_subject": 234, "accuracy": 0.8917748917748918, "num_readable_responses": 231, "input_tokens_avg": 775.6709956709957}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.8095238095238095, "num_readable_responses": 63, "input_tokens_avg": 860.6666666666666}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.8383233532934131, "num_readable_responses": 668, "input_tokens_avg": 603.312874251497}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.7054794520547946, "num_readable_responses": 292, "input_tokens_avg": 823.0719178082192}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.5718085106382979, "num_readable_responses": 376, "input_tokens_avg": 1282.718085106383}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.6932773109243697, "num_readable_responses": 238, "input_tokens_avg": 831.9495798319327}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.7337883959044369, "num_readable_responses": 293, "input_tokens_avg": 947.8327645051195}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.6936026936026936, "num_readable_responses": 297, "input_tokens_avg": 968.7070707070707}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.53125, "num_readable_responses": 96, "input_tokens_avg": 1397.9166666666667}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.75, "num_readable_responses": 4, "input_tokens_avg": 2268.75}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.6627906976744186, "num_readable_responses": 172, "input_tokens_avg": 1917.2616279069769}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.6162790697674418, "num_readable_responses": 602, "input_tokens_avg": 1148.8637873754153}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.7551020408163265, "num_readable_responses": 98, "input_tokens_avg": 758.4897959183673}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.6958333333333333, "num_readable_responses": 240, "input_tokens_avg": 1870.3}, "sociology": {"num_questions_subject": 201, "accuracy": 0.8578680203045685, "num_readable_responses": 197, "input_tokens_avg": 867.1015228426396}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.8247422680412371, "num_readable_responses": 97, "input_tokens_avg": 936.6494845360825}, "virology": {"num_questions_subject": 166, "accuracy": 0.5126582278481012, "num_readable_responses": 158, "input_tokens_avg": 1036.367088607595}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.8260869565217391, "num_readable_responses": 161, "input_tokens_avg": 633.1180124223603}}}