{"model": "meta-llama/Meta-Llama-3-8B-Instruct", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 128256, "max_position_embeddings": 8192, "hidden_size": 4096, "intermediate_size": 14336, "num_hidden_layers": 32, "num_attention_heads": 32, "num_key_value_heads": 8, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "pretraining_tp": 1, "use_cache": true, "rope_theta": 500000.0, "rope_scaling": null, "attention_bias": false, "attention_dropout": 0.0, "mlp_bias": false, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["LlamaForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 128000, "pad_token_id": null, "eos_token_id": 128009, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "meta-llama/Meta-Llama-3-8B-Instruct", "transformers_version": "4.42.3", "model_type": "llama"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.34375, "num_readable_responses": 64, "input_tokens_avg": 702.8125}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.6484375, "num_readable_responses": 128, "input_tokens_avg": 743.78125}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.7310924369747899, "num_readable_responses": 119, "input_tokens_avg": 883.344537815126}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.6712328767123288, "num_readable_responses": 73, "input_tokens_avg": 782.1095890410959}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.7808219178082192, "num_readable_responses": 219, "input_tokens_avg": 647.648401826484}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.7555555555555555, "num_readable_responses": 135, "input_tokens_avg": 805.5703703703704}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.6140350877192983, "num_readable_responses": 57, "input_tokens_avg": 934.6315789473684}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.5949367088607594, "num_readable_responses": 79, "input_tokens_avg": 1038.9367088607594}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.2857142857142857, "num_readable_responses": 49, "input_tokens_avg": 975.2244897959183}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.6304347826086957, "num_readable_responses": 138, "input_tokens_avg": 885.072463768116}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.6341463414634146, "num_readable_responses": 41, "input_tokens_avg": 875.9512195121952}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.7912087912087912, "num_readable_responses": 91, "input_tokens_avg": 1043.2967032967033}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.5876288659793815, "num_readable_responses": 194, "input_tokens_avg": 580.6237113402062}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.5238095238095238, "num_readable_responses": 105, "input_tokens_avg": 960.9047619047619}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.6608695652173913, "num_readable_responses": 115, "input_tokens_avg": 642.5739130434782}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 0.6290322580645161, "num_readable_responses": 62, "input_tokens_avg": 789.983870967742}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.45454545454545453, "num_readable_responses": 121, "input_tokens_avg": 1184.01652892562}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.38461538461538464, "num_readable_responses": 26, "input_tokens_avg": 678.8846153846154}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.8, "num_readable_responses": 290, "input_tokens_avg": 955.5379310344828}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.593103448275862, "num_readable_responses": 145, "input_tokens_avg": 907.6896551724138}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.7631578947368421, "num_readable_responses": 76, "input_tokens_avg": 941.3947368421053}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.7516339869281046, "num_readable_responses": 153, "input_tokens_avg": 3418.326797385621}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.8402366863905325, "num_readable_responses": 169, "input_tokens_avg": 620.603550295858}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.8926553672316384, "num_readable_responses": 177, "input_tokens_avg": 850.7175141242938}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.6554621848739496, "num_readable_responses": 357, "input_tokens_avg": 799.1848739495798}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.17307692307692307, "num_readable_responses": 52, "input_tokens_avg": 806.25}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.7079646017699115, "num_readable_responses": 226, "input_tokens_avg": 732.7035398230089}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.36046511627906974, "num_readable_responses": 86, "input_tokens_avg": 1055.6976744186047}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.8313008130081301, "num_readable_responses": 492, "input_tokens_avg": 703.8780487804878}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.543046357615894, "num_readable_responses": 151, "input_tokens_avg": 1214.3245033112582}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.8043478260869565, "num_readable_responses": 184, "input_tokens_avg": 2774.0489130434785}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.8371040723981901, "num_readable_responses": 221, "input_tokens_avg": 3166.923076923077}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.7209302325581395, "num_readable_responses": 215, "input_tokens_avg": 615.1860465116279}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.7589285714285714, "num_readable_responses": 112, "input_tokens_avg": 643.9107142857143}, "international_law": {"num_questions_subject": 121, "accuracy": 0.7692307692307693, "num_readable_responses": 117, "input_tokens_avg": 1047.4444444444443}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.7403846153846154, "num_readable_responses": 104, "input_tokens_avg": 771.6153846153846}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.7466666666666667, "num_readable_responses": 150, "input_tokens_avg": 795.7466666666667}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.48, "num_readable_responses": 100, "input_tokens_avg": 797.69}, "management": {"num_questions_subject": 103, "accuracy": 0.898876404494382, "num_readable_responses": 89, "input_tokens_avg": 507.7415730337079}, "marketing": {"num_questions_subject": 234, "accuracy": 0.8913043478260869, "num_readable_responses": 230, "input_tokens_avg": 643.8521739130434}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.8205128205128205, "num_readable_responses": 78, "input_tokens_avg": 742.9230769230769}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.8461538461538461, "num_readable_responses": 650, "input_tokens_avg": 523.4292307692308}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.7046783625730995, "num_readable_responses": 342, "input_tokens_avg": 719.2309941520468}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.4352392065344224, "num_readable_responses": 857, "input_tokens_avg": 1109.3955659276546}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.7569444444444444, "num_readable_responses": 288, "input_tokens_avg": 696.2152777777778}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.762987012987013, "num_readable_responses": 308, "input_tokens_avg": 826.3506493506494}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.7292418772563177, "num_readable_responses": 277, "input_tokens_avg": 807.173285198556}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.592391304347826, "num_readable_responses": 184, "input_tokens_avg": 1164.0923913043478}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.44818481848184816, "num_readable_responses": 1515, "input_tokens_avg": 2163.2884488448844}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.6910569105691057, "num_readable_responses": 246, "input_tokens_avg": 1636.8536585365853}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.7066666666666667, "num_readable_responses": 600, "input_tokens_avg": 997.3633333333333}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.6702127659574468, "num_readable_responses": 94, "input_tokens_avg": 667.6808510638298}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.7363636363636363, "num_readable_responses": 220, "input_tokens_avg": 1644.4863636363636}, "sociology": {"num_questions_subject": 201, "accuracy": 0.8248587570621468, "num_readable_responses": 177, "input_tokens_avg": 773.9435028248588}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.8602150537634409, "num_readable_responses": 93, "input_tokens_avg": 830.4623655913979}, "virology": {"num_questions_subject": 166, "accuracy": 0.54375, "num_readable_responses": 160, "input_tokens_avg": 831.14375}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.8169934640522876, "num_readable_responses": 153, "input_tokens_avg": 541.0522875816994}}}