{"model": "mistralai/Mistral-7B-Instruct-v0.3", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 32768, "max_position_embeddings": 32768, "hidden_size": 4096, "intermediate_size": 14336, "num_hidden_layers": 32, "num_attention_heads": 32, "sliding_window": null, "head_dim": 128, "num_key_value_heads": 8, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "use_cache": true, "rope_theta": 1000000.0, "attention_dropout": 0.0, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["MistralForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 1, "pad_token_id": null, "eos_token_id": 2, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "mistralai/Mistral-7B-Instruct-v0.3", "transformers_version": "4.43.3", "model_type": "mistral"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.26865671641791045, "num_readable_responses": 67, "input_tokens_avg": 789.1044776119403}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.5851851851851851, "num_readable_responses": 135, "input_tokens_avg": 874.1407407407407}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.689922480620155, "num_readable_responses": 129, "input_tokens_avg": 1034.6976744186047}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.686046511627907, "num_readable_responses": 86, "input_tokens_avg": 918.453488372093}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.7081545064377682, "num_readable_responses": 233, "input_tokens_avg": 777.6566523605151}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.6928571428571428, "num_readable_responses": 140, "input_tokens_avg": 933.7285714285714}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.6545454545454545, "num_readable_responses": 55, "input_tokens_avg": 1086.1272727272728}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.48, "num_readable_responses": 75, "input_tokens_avg": 1175.52}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.3617021276595745, "num_readable_responses": 47, "input_tokens_avg": 1111.5106382978724}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.6578947368421053, "num_readable_responses": 152, "input_tokens_avg": 1066.953947368421}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.675, "num_readable_responses": 40, "input_tokens_avg": 998.775}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.75, "num_readable_responses": 88, "input_tokens_avg": 1211.215909090909}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.5528846153846154, "num_readable_responses": 208, "input_tokens_avg": 666.0240384615385}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.44339622641509435, "num_readable_responses": 106, "input_tokens_avg": 1097.632075471698}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.6071428571428571, "num_readable_responses": 112, "input_tokens_avg": 727.2142857142857}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 0.6, "num_readable_responses": 50, "input_tokens_avg": 903.98}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.4365079365079365, "num_readable_responses": 126, "input_tokens_avg": 1420.373015873016}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.3875, "num_readable_responses": 80, "input_tokens_avg": 808.375}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.7591973244147158, "num_readable_responses": 299, "input_tokens_avg": 1110.1672240802675}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.5424836601307189, "num_readable_responses": 153, "input_tokens_avg": 1065.6013071895425}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.7532467532467533, "num_readable_responses": 77, "input_tokens_avg": 1077.7012987012988}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.7575757575757576, "num_readable_responses": 165, "input_tokens_avg": 3779.109090909091}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.7783505154639175, "num_readable_responses": 194, "input_tokens_avg": 730.340206185567}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.8860103626943006, "num_readable_responses": 193, "input_tokens_avg": 988.9326424870467}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.6432432432432432, "num_readable_responses": 370, "input_tokens_avg": 925.4864864864865}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.4, "num_readable_responses": 50, "input_tokens_avg": 943.4}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.6708860759493671, "num_readable_responses": 237, "input_tokens_avg": 852.5569620253165}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.391304347826087, "num_readable_responses": 92, "input_tokens_avg": 1243.445652173913}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.8342749529190208, "num_readable_responses": 531, "input_tokens_avg": 831.3596986817325}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.5641025641025641, "num_readable_responses": 156, "input_tokens_avg": 1379.070512820513}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.8235294117647058, "num_readable_responses": 204, "input_tokens_avg": 3133.4460784313724}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.7711864406779662, "num_readable_responses": 236, "input_tokens_avg": 3561.334745762712}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.7018348623853211, "num_readable_responses": 218, "input_tokens_avg": 699.4678899082569}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.7933884297520661, "num_readable_responses": 121, "input_tokens_avg": 756.2561983471074}, "international_law": {"num_questions_subject": 121, "accuracy": 0.7851239669421488, "num_readable_responses": 121, "input_tokens_avg": 1166.1818181818182}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.7592592592592593, "num_readable_responses": 108, "input_tokens_avg": 911.2777777777778}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.7668711656441718, "num_readable_responses": 163, "input_tokens_avg": 873.8895705521472}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.4117647058823529, "num_readable_responses": 102, "input_tokens_avg": 903.2941176470588}, "management": {"num_questions_subject": 103, "accuracy": 0.8333333333333334, "num_readable_responses": 102, "input_tokens_avg": 585.5294117647059}, "marketing": {"num_questions_subject": 234, "accuracy": 0.8675213675213675, "num_readable_responses": 234, "input_tokens_avg": 775.8974358974359}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.8048780487804879, "num_readable_responses": 82, "input_tokens_avg": 860.4634146341464}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.8295774647887324, "num_readable_responses": 710, "input_tokens_avg": 603.7028169014085}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.6985507246376812, "num_readable_responses": 345, "input_tokens_avg": 824.1159420289855}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.43575418994413406, "num_readable_responses": 895, "input_tokens_avg": 1283.5162011173184}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.7137931034482758, "num_readable_responses": 290, "input_tokens_avg": 835.5310344827586}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.7451612903225806, "num_readable_responses": 310, "input_tokens_avg": 947.3387096774194}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.7275747508305648, "num_readable_responses": 301, "input_tokens_avg": 967.9269102990033}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.5598086124401914, "num_readable_responses": 209, "input_tokens_avg": 1397.8803827751196}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.4607329842931937, "num_readable_responses": 1528, "input_tokens_avg": 2454.7905759162304}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.6804511278195489, "num_readable_responses": 266, "input_tokens_avg": 1922.7105263157894}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.6727272727272727, "num_readable_responses": 605, "input_tokens_avg": 1148.7107438016528}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.693069306930693, "num_readable_responses": 101, "input_tokens_avg": 759.2079207920792}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.7272727272727273, "num_readable_responses": 242, "input_tokens_avg": 1870.3471074380166}, "sociology": {"num_questions_subject": 201, "accuracy": 0.85, "num_readable_responses": 200, "input_tokens_avg": 867.26}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.8673469387755102, "num_readable_responses": 98, "input_tokens_avg": 936.704081632653}, "virology": {"num_questions_subject": 166, "accuracy": 0.5153374233128835, "num_readable_responses": 163, "input_tokens_avg": 1036.0613496932515}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.8098159509202454, "num_readable_responses": 163, "input_tokens_avg": 633.2760736196319}}}