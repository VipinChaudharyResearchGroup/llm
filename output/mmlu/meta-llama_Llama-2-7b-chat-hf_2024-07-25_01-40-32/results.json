{"model": "meta-llama/Llama-2-7b-chat-hf", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 32000, "max_position_embeddings": 4096, "hidden_size": 4096, "intermediate_size": 11008, "num_hidden_layers": 32, "num_attention_heads": 32, "num_key_value_heads": 32, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "pretraining_tp": 1, "use_cache": true, "rope_theta": 10000.0, "rope_scaling": null, "attention_bias": false, "attention_dropout": 0.0, "mlp_bias": false, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["LlamaForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 1, "pad_token_id": null, "eos_token_id": 2, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "meta-llama/Llama-2-7b-chat-hf", "transformers_version": "4.42.3", "model_type": "llama"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.24193548387096775, "num_readable_responses": 62, "input_tokens_avg": 783.5322580645161}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.41353383458646614, "num_readable_responses": 133, "input_tokens_avg": 890.3684210526316}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.4666666666666667, "num_readable_responses": 120, "input_tokens_avg": 1049.9333333333334}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.573170731707317, "num_readable_responses": 82, "input_tokens_avg": 963.0}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.4975369458128079, "num_readable_responses": 203, "input_tokens_avg": 792.1182266009852}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.6016949152542372, "num_readable_responses": 118, "input_tokens_avg": 947.957627118644}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.4444444444444444, "num_readable_responses": 36, "input_tokens_avg": 1117.361111111111}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.4745762711864407, "num_readable_responses": 59, "input_tokens_avg": 1168.6101694915253}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.3333333333333333, "num_readable_responses": 36, "input_tokens_avg": 1105.1944444444443}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.41304347826086957, "num_readable_responses": 138, "input_tokens_avg": 1085.623188405797}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.37142857142857144, "num_readable_responses": 35, "input_tokens_avg": 1001.5428571428571}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.6133333333333333, "num_readable_responses": 75, "input_tokens_avg": 1221.9733333333334}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.38341968911917096, "num_readable_responses": 193, "input_tokens_avg": 687.6735751295337}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.31958762886597936, "num_readable_responses": 97, "input_tokens_avg": 1097.1340206185566}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.49514563106796117, "num_readable_responses": 103, "input_tokens_avg": 740.1165048543689}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 0.3274336283185841, "num_readable_responses": 113, "input_tokens_avg": 912.9911504424779}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.30578512396694213, "num_readable_responses": 121, "input_tokens_avg": 1361.1239669421489}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.35135135135135137, "num_readable_responses": 37, "input_tokens_avg": 821.8108108108108}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.5810276679841897, "num_readable_responses": 253, "input_tokens_avg": 1129.300395256917}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.3971631205673759, "num_readable_responses": 141, "input_tokens_avg": 1081.9219858156027}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.44, "num_readable_responses": 75, "input_tokens_avg": 1092.44}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.625, "num_readable_responses": 112, "input_tokens_avg": 3838.0178571428573}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.6428571428571429, "num_readable_responses": 182, "input_tokens_avg": 746.0934065934066}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.7443181818181818, "num_readable_responses": 176, "input_tokens_avg": 1029.0397727272727}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.47023809523809523, "num_readable_responses": 336, "input_tokens_avg": 972.5714285714286}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.27, "num_readable_responses": 100, "input_tokens_avg": 940.33}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.41232227488151657, "num_readable_responses": 211, "input_tokens_avg": 872.7725118483412}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.23076923076923078, "num_readable_responses": 91, "input_tokens_avg": 1252.2197802197802}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.6973947895791583, "num_readable_responses": 499, "input_tokens_avg": 849.4929859719439}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.3815789473684211, "num_readable_responses": 152, "input_tokens_avg": 1389.3684210526317}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.6606060606060606, "num_readable_responses": 165, "input_tokens_avg": 3197.6242424242423}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.7151162790697675, "num_readable_responses": 172, "input_tokens_avg": 3699.0523255813955}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.5821596244131455, "num_readable_responses": 213, "input_tokens_avg": 717.5117370892019}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.5188679245283019, "num_readable_responses": 106, "input_tokens_avg": 779.1603773584906}, "international_law": {"num_questions_subject": 121, "accuracy": 0.6521739130434783, "num_readable_responses": 115, "input_tokens_avg": 1198.4173913043478}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.5959595959595959, "num_readable_responses": 99, "input_tokens_avg": 928.7474747474747}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.5900621118012422, "num_readable_responses": 161, "input_tokens_avg": 913.8819875776397}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.3829787234042553, "num_readable_responses": 94, "input_tokens_avg": 908.7659574468086}, "management": {"num_questions_subject": 103, "accuracy": 0.6565656565656566, "num_readable_responses": 99, "input_tokens_avg": 594.10101010101}, "marketing": {"num_questions_subject": 234, "accuracy": 0.7207207207207207, "num_readable_responses": 222, "input_tokens_avg": 826.5045045045046}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.6410256410256411, "num_readable_responses": 78, "input_tokens_avg": 872.6153846153846}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.7281690140845071, "num_readable_responses": 710, "input_tokens_avg": 614.025352112676}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.5029940119760479, "num_readable_responses": 334, "input_tokens_avg": 840.6616766467066}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.2617611580217129, "num_readable_responses": 829, "input_tokens_avg": 1304.4366706875753}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.5604838709677419, "num_readable_responses": 248, "input_tokens_avg": 863.2096774193549}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.6051779935275081, "num_readable_responses": 309, "input_tokens_avg": 978.7766990291262}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.6170212765957447, "num_readable_responses": 282, "input_tokens_avg": 972.2021276595744}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.39655172413793105, "num_readable_responses": 174, "input_tokens_avg": 1436.0747126436781}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.421259842519685, "num_readable_responses": 254, "input_tokens_avg": 2536.696850393701}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.4489795918367347, "num_readable_responses": 196, "input_tokens_avg": 1992.5969387755101}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.4672268907563025, "num_readable_responses": 595, "input_tokens_avg": 1194.8840336134454}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.5806451612903226, "num_readable_responses": 93, "input_tokens_avg": 784.7096774193549}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.5069124423963134, "num_readable_responses": 217, "input_tokens_avg": 1953.004608294931}, "sociology": {"num_questions_subject": 201, "accuracy": 0.7142857142857143, "num_readable_responses": 196, "input_tokens_avg": 888.8520408163265}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.7346938775510204, "num_readable_responses": 98, "input_tokens_avg": 962.469387755102}, "virology": {"num_questions_subject": 166, "accuracy": 0.43125, "num_readable_responses": 160, "input_tokens_avg": 1071.4625}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.7169811320754716, "num_readable_responses": 159, "input_tokens_avg": 638.8616352201258}}}