{"model": "meta-llama/Llama-2-7b-hf", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 32000, "max_position_embeddings": 4096, "hidden_size": 4096, "intermediate_size": 11008, "num_hidden_layers": 32, "num_attention_heads": 32, "num_key_value_heads": 32, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "pretraining_tp": 1, "use_cache": true, "rope_theta": 10000.0, "rope_scaling": null, "attention_bias": false, "attention_dropout": 0.0, "mlp_bias": false, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["LlamaForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 1, "pad_token_id": null, "eos_token_id": 2, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "meta-llama/Llama-2-7b-hf", "transformers_version": "4.42.3", "model_type": "llama"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.28205128205128205, "num_readable_responses": 39, "input_tokens_avg": 785.1282051282051}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.5454545454545454, "num_readable_responses": 33, "input_tokens_avg": 892.4848484848485}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.5128205128205128, "num_readable_responses": 39, "input_tokens_avg": 1042.3076923076924}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.4583333333333333, "num_readable_responses": 24, "input_tokens_avg": 970.25}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.4691358024691358, "num_readable_responses": 81, "input_tokens_avg": 791.0864197530864}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.42857142857142855, "num_readable_responses": 63, "input_tokens_avg": 949.936507936508}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.3, "num_readable_responses": 20, "input_tokens_avg": 1115.85}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.5, "num_readable_responses": 24, "input_tokens_avg": 1171.4583333333333}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.46153846153846156, "num_readable_responses": 13, "input_tokens_avg": 1113.8461538461538}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.47368421052631576, "num_readable_responses": 38, "input_tokens_avg": 1130.2894736842106}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.45454545454545453, "num_readable_responses": 11, "input_tokens_avg": 999.8181818181819}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.6111111111111112, "num_readable_responses": 18, "input_tokens_avg": 1212.8333333333333}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.34265734265734266, "num_readable_responses": 143, "input_tokens_avg": 686.7762237762238}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.3333333333333333, "num_readable_responses": 33, "input_tokens_avg": 1085.060606060606}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.3, "num_readable_responses": 40, "input_tokens_avg": 739.575}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 0.125, "num_readable_responses": 8, "input_tokens_avg": 929.875}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.27419354838709675, "num_readable_responses": 62, "input_tokens_avg": 1362.6129032258063}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.4166666666666667, "num_readable_responses": 12, "input_tokens_avg": 828.25}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.4230769230769231, "num_readable_responses": 130, "input_tokens_avg": 1128.8153846153846}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.3076923076923077, "num_readable_responses": 39, "input_tokens_avg": 1084.3333333333333}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.45454545454545453, "num_readable_responses": 33, "input_tokens_avg": 1098.4242424242425}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.4444444444444444, "num_readable_responses": 45, "input_tokens_avg": 3882.5555555555557}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.46153846153846156, "num_readable_responses": 78, "input_tokens_avg": 748.6153846153846}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.631578947368421, "num_readable_responses": 76, "input_tokens_avg": 1030.0}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.375, "num_readable_responses": 144, "input_tokens_avg": 979.8819444444445}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.3, "num_readable_responses": 20, "input_tokens_avg": 940.4}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.4146341463414634, "num_readable_responses": 82, "input_tokens_avg": 873.7439024390244}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.22580645161290322, "num_readable_responses": 31, "input_tokens_avg": 1245.6774193548388}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.6666666666666666, "num_readable_responses": 225, "input_tokens_avg": 848.3644444444444}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.4358974358974359, "num_readable_responses": 39, "input_tokens_avg": 1390.6923076923076}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.45901639344262296, "num_readable_responses": 61, "input_tokens_avg": 3203.5737704918033}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.4583333333333333, "num_readable_responses": 72, "input_tokens_avg": 3673.4166666666665}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.4336283185840708, "num_readable_responses": 113, "input_tokens_avg": 718.0619469026549}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.5, "num_readable_responses": 44, "input_tokens_avg": 785.7272727272727}, "international_law": {"num_questions_subject": 121, "accuracy": 0.475, "num_readable_responses": 40, "input_tokens_avg": 1199.825}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0, "num_readable_responses": 0, "input_tokens_avg": 0}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.5350877192982456, "num_readable_responses": 114, "input_tokens_avg": 912.0877192982456}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.2, "num_readable_responses": 50, "input_tokens_avg": 915.16}, "management": {"num_questions_subject": 103, "accuracy": 0.5769230769230769, "num_readable_responses": 52, "input_tokens_avg": 594.6153846153846}, "marketing": {"num_questions_subject": 234, "accuracy": 0.6836734693877551, "num_readable_responses": 98, "input_tokens_avg": 825.3061224489796}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.5128205128205128, "num_readable_responses": 39, "input_tokens_avg": 872.9230769230769}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.6798866855524079, "num_readable_responses": 353, "input_tokens_avg": 609.4305949008499}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.42168674698795183, "num_readable_responses": 249, "input_tokens_avg": 840.3493975903615}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.2573913043478261, "num_readable_responses": 575, "input_tokens_avg": 1304.4313043478262}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.47058823529411764, "num_readable_responses": 85, "input_tokens_avg": 865.4235294117647}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.5073891625615764, "num_readable_responses": 203, "input_tokens_avg": 981.5714285714286}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.5462962962962963, "num_readable_responses": 108, "input_tokens_avg": 974.1388888888889}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.3684210526315789, "num_readable_responses": 38, "input_tokens_avg": 1431.921052631579}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0, "num_readable_responses": 0, "input_tokens_avg": 0}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.4852941176470588, "num_readable_responses": 68, "input_tokens_avg": 1993.9558823529412}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.3946784922394678, "num_readable_responses": 451, "input_tokens_avg": 1194.8580931263857}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.5862068965517241, "num_readable_responses": 29, "input_tokens_avg": 777.1034482758621}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.43478260869565216, "num_readable_responses": 92, "input_tokens_avg": 1931.5108695652175}, "sociology": {"num_questions_subject": 201, "accuracy": 0.6101694915254238, "num_readable_responses": 59, "input_tokens_avg": 892.2881355932203}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.5319148936170213, "num_readable_responses": 47, "input_tokens_avg": 962.3404255319149}, "virology": {"num_questions_subject": 166, "accuracy": 0.46226415094339623, "num_readable_responses": 106, "input_tokens_avg": 1070.5566037735848}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.6171875, "num_readable_responses": 128, "input_tokens_avg": 638.9609375}}}