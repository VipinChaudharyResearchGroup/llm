{"model": "mistralai/Mistral-7B-v0.1", "generate_kwargs": {"num_return_sequences": 1, "max_new_tokens": 24}, "model_config": {"vocab_size": 32000, "max_position_embeddings": 32768, "hidden_size": 4096, "intermediate_size": 14336, "num_hidden_layers": 32, "num_attention_heads": 32, "sliding_window": 4096, "num_key_value_heads": 8, "hidden_act": "silu", "initializer_range": 0.02, "rms_norm_eps": 1e-05, "use_cache": true, "rope_theta": 10000.0, "attention_dropout": 0.0, "return_dict": true, "output_hidden_states": false, "output_attentions": false, "torchscript": false, "torch_dtype": "float16", "use_bfloat16": false, "tf_legacy_loss": false, "pruned_heads": {}, "tie_word_embeddings": false, "chunk_size_feed_forward": 0, "is_encoder_decoder": false, "is_decoder": false, "cross_attention_hidden_size": null, "add_cross_attention": false, "tie_encoder_decoder": false, "max_length": 20, "min_length": 0, "do_sample": false, "early_stopping": false, "num_beams": 1, "num_beam_groups": 1, "diversity_penalty": 0.0, "temperature": 1.0, "top_k": 50, "top_p": 1.0, "typical_p": 1.0, "repetition_penalty": 1.0, "length_penalty": 1.0, "no_repeat_ngram_size": 0, "encoder_no_repeat_ngram_size": 0, "bad_words_ids": null, "num_return_sequences": 1, "output_scores": false, "return_dict_in_generate": false, "forced_bos_token_id": null, "forced_eos_token_id": null, "remove_invalid_values": false, "exponential_decay_length_penalty": null, "suppress_tokens": null, "begin_suppress_tokens": null, "architectures": ["MistralForCausalLM"], "finetuning_task": null, "id2label": {"0": "LABEL_0", "1": "LABEL_1"}, "label2id": {"LABEL_0": 0, "LABEL_1": 1}, "tokenizer_class": null, "prefix": null, "bos_token_id": 1, "pad_token_id": null, "eos_token_id": 2, "sep_token_id": null, "decoder_start_token_id": null, "task_specific_params": null, "problem_type": null, "_name_or_path": "mistralai/Mistral-7B-v0.1", "transformers_version": "4.42.3", "model_type": "mistral"}, "subjects_results": {"abstract_algebra": {"num_questions_subject": 100, "accuracy": 0.2786885245901639, "num_readable_responses": 61, "input_tokens_avg": 789.3934426229508}, "anatomy": {"num_questions_subject": 135, "accuracy": 0.6287878787878788, "num_readable_responses": 132, "input_tokens_avg": 874.0681818181819}, "astronomy": {"num_questions_subject": 152, "accuracy": 0.7027027027027027, "num_readable_responses": 111, "input_tokens_avg": 1035.981981981982}, "business_ethics": {"num_questions_subject": 100, "accuracy": 0.6862745098039216, "num_readable_responses": 51, "input_tokens_avg": 930.6666666666666}, "clinical_knowledge": {"num_questions_subject": 265, "accuracy": 0.76, "num_readable_responses": 200, "input_tokens_avg": 777.705}, "college_biology": {"num_questions_subject": 144, "accuracy": 0.7218045112781954, "num_readable_responses": 133, "input_tokens_avg": 934.6691729323309}, "college_chemistry": {"num_questions_subject": 100, "accuracy": 0.5957446808510638, "num_readable_responses": 47, "input_tokens_avg": 1084.2553191489362}, "college_computer_science": {"num_questions_subject": 100, "accuracy": 0.4528301886792453, "num_readable_responses": 53, "input_tokens_avg": 1176.433962264151}, "college_mathematics": {"num_questions_subject": 100, "accuracy": 0.4523809523809524, "num_readable_responses": 42, "input_tokens_avg": 1110.2142857142858}, "college_medicine": {"num_questions_subject": 173, "accuracy": 0.625, "num_readable_responses": 96, "input_tokens_avg": 1089.6666666666667}, "college_physics": {"num_questions_subject": 102, "accuracy": 0.575, "num_readable_responses": 40, "input_tokens_avg": 999.225}, "computer_security": {"num_questions_subject": 100, "accuracy": 0.9111111111111111, "num_readable_responses": 45, "input_tokens_avg": 1198.2666666666667}, "conceptual_physics": {"num_questions_subject": 235, "accuracy": 0.5921787709497207, "num_readable_responses": 179, "input_tokens_avg": 665.0837988826815}, "econometrics": {"num_questions_subject": 114, "accuracy": 0.5048543689320388, "num_readable_responses": 103, "input_tokens_avg": 1097.0485436893205}, "electrical_engineering": {"num_questions_subject": 145, "accuracy": 0.6354166666666666, "num_readable_responses": 96, "input_tokens_avg": 727.0208333333334}, "elementary_mathematics": {"num_questions_subject": 378, "accuracy": 1.0, "num_readable_responses": 8, "input_tokens_avg": 875.375}, "formal_logic": {"num_questions_subject": 126, "accuracy": 0.373015873015873, "num_readable_responses": 126, "input_tokens_avg": 1420.373015873016}, "global_facts": {"num_questions_subject": 100, "accuracy": 0.3548387096774194, "num_readable_responses": 31, "input_tokens_avg": 812.9354838709677}, "high_school_biology": {"num_questions_subject": 310, "accuracy": 0.7444933920704846, "num_readable_responses": 227, "input_tokens_avg": 1112.4625550660794}, "high_school_chemistry": {"num_questions_subject": 203, "accuracy": 0.5833333333333334, "num_readable_responses": 120, "input_tokens_avg": 1063.95}, "high_school_computer_science": {"num_questions_subject": 100, "accuracy": 0.6825396825396826, "num_readable_responses": 63, "input_tokens_avg": 1082.2698412698412}, "high_school_european_history": {"num_questions_subject": 165, "accuracy": 0.7484662576687117, "num_readable_responses": 163, "input_tokens_avg": 3780.472392638037}, "high_school_geography": {"num_questions_subject": 198, "accuracy": 0.7771739130434783, "num_readable_responses": 184, "input_tokens_avg": 730.6576086956521}, "high_school_government_and_politics": {"num_questions_subject": 193, "accuracy": 0.8715083798882681, "num_readable_responses": 179, "input_tokens_avg": 989.5754189944134}, "high_school_macroeconomics": {"num_questions_subject": 390, "accuracy": 0.6560509554140127, "num_readable_responses": 314, "input_tokens_avg": 925.0127388535032}, "high_school_mathematics": {"num_questions_subject": 270, "accuracy": 0.3333333333333333, "num_readable_responses": 3, "input_tokens_avg": 970.3333333333334}, "high_school_microeconomics": {"num_questions_subject": 238, "accuracy": 0.6816143497757847, "num_readable_responses": 223, "input_tokens_avg": 851.3946188340807}, "high_school_physics": {"num_questions_subject": 151, "accuracy": 0.3563218390804598, "num_readable_responses": 87, "input_tokens_avg": 1243.9885057471265}, "high_school_psychology": {"num_questions_subject": 545, "accuracy": 0.7965686274509803, "num_readable_responses": 408, "input_tokens_avg": 832.7009803921569}, "high_school_statistics": {"num_questions_subject": 216, "accuracy": 0.5104895104895105, "num_readable_responses": 143, "input_tokens_avg": 1379.1118881118882}, "high_school_us_history": {"num_questions_subject": 204, "accuracy": 0.785, "num_readable_responses": 200, "input_tokens_avg": 3133.71}, "high_school_world_history": {"num_questions_subject": 237, "accuracy": 0.7649572649572649, "num_readable_responses": 234, "input_tokens_avg": 3560.017094017094}, "human_aging": {"num_questions_subject": 223, "accuracy": 0.7476635514018691, "num_readable_responses": 214, "input_tokens_avg": 699.5093457943925}, "human_sexuality": {"num_questions_subject": 131, "accuracy": 0.8421052631578947, "num_readable_responses": 114, "input_tokens_avg": 756.9736842105264}, "international_law": {"num_questions_subject": 121, "accuracy": 0.811965811965812, "num_readable_responses": 117, "input_tokens_avg": 1166.4786324786326}, "jurisprudence": {"num_questions_subject": 108, "accuracy": 0.7659574468085106, "num_readable_responses": 94, "input_tokens_avg": 909.6702127659574}, "logical_fallacies": {"num_questions_subject": 163, "accuracy": 0.7654320987654321, "num_readable_responses": 162, "input_tokens_avg": 873.6975308641976}, "machine_learning": {"num_questions_subject": 112, "accuracy": 0.7027027027027027, "num_readable_responses": 37, "input_tokens_avg": 895.2162162162163}, "management": {"num_questions_subject": 103, "accuracy": 0.8217821782178217, "num_readable_responses": 101, "input_tokens_avg": 585.5742574257425}, "marketing": {"num_questions_subject": 234, "accuracy": 0.8583690987124464, "num_readable_responses": 233, "input_tokens_avg": 775.7854077253219}, "medical_genetics": {"num_questions_subject": 100, "accuracy": 0.7681159420289855, "num_readable_responses": 69, "input_tokens_avg": 858.304347826087}, "miscellaneous": {"num_questions_subject": 783, "accuracy": 0.8433931484502447, "num_readable_responses": 613, "input_tokens_avg": 603.5644371941272}, "moral_disputes": {"num_questions_subject": 346, "accuracy": 0.7246376811594203, "num_readable_responses": 345, "input_tokens_avg": 824.1159420289855}, "moral_scenarios": {"num_questions_subject": 895, "accuracy": 0.4232922732362822, "num_readable_responses": 893, "input_tokens_avg": 1283.521836506159}, "nutrition": {"num_questions_subject": 306, "accuracy": 0.7560975609756098, "num_readable_responses": 287, "input_tokens_avg": 835.1498257839721}, "philosophy": {"num_questions_subject": 311, "accuracy": 0.7170418006430869, "num_readable_responses": 311, "input_tokens_avg": 947.2668810289389}, "prehistory": {"num_questions_subject": 324, "accuracy": 0.75, "num_readable_responses": 300, "input_tokens_avg": 967.9566666666667}, "professional_accounting": {"num_questions_subject": 282, "accuracy": 0.5625, "num_readable_responses": 144, "input_tokens_avg": 1395.2847222222222}, "professional_law": {"num_questions_subject": 1534, "accuracy": 0.7676767676767676, "num_readable_responses": 99, "input_tokens_avg": 2500.5757575757575}, "professional_medicine": {"num_questions_subject": 272, "accuracy": 0.6704119850187266, "num_readable_responses": 267, "input_tokens_avg": 1922.681647940075}, "professional_psychology": {"num_questions_subject": 612, "accuracy": 0.6716171617161716, "num_readable_responses": 606, "input_tokens_avg": 1148.6567656765676}, "public_relations": {"num_questions_subject": 110, "accuracy": 0.6853932584269663, "num_readable_responses": 89, "input_tokens_avg": 760.1685393258427}, "security_studies": {"num_questions_subject": 245, "accuracy": 0.7361702127659574, "num_readable_responses": 235, "input_tokens_avg": 1873.2340425531916}, "sociology": {"num_questions_subject": 201, "accuracy": 0.855, "num_readable_responses": 200, "input_tokens_avg": 867.26}, "us_foreign_policy": {"num_questions_subject": 100, "accuracy": 0.8865979381443299, "num_readable_responses": 97, "input_tokens_avg": 936.4432989690722}, "virology": {"num_questions_subject": 166, "accuracy": 0.5214723926380368, "num_readable_responses": 163, "input_tokens_avg": 1035.9815950920245}, "world_religions": {"num_questions_subject": 171, "accuracy": 0.8385093167701864, "num_readable_responses": 161, "input_tokens_avg": 633.1180124223603}}}